{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "strong-scientist",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Serializer:\n",
    "    def __init__(self, possible_labels):\n",
    "        self.allowed_chars_in_filtered_name = [\n",
    "            ' ', 'a', 'b', 'c', \n",
    "            'd', 'e', 'f', 'g', \n",
    "            'h', 'i', 'j', 'k', \n",
    "            'l', 'm', 'n', 'o', \n",
    "            'p', 'q', 'r', 's', \n",
    "            't', 'u', 'v', 'w', \n",
    "            'x', 'y', 'z', 'á', \n",
    "            'ã', 'ä', 'ç', 'è', \n",
    "            'é', 'ë', 'ï', 'ô', \n",
    "            'ö', 'ü', '-'#,         <- We remove '$' and '+' for this matter\n",
    "            #$', '+',\n",
    "        ]\n",
    "\n",
    "        self.allowed_chars_in_name = set([\n",
    "            ' ', 'a', 'b', 'c', \n",
    "            'd', 'e', 'f', 'g', \n",
    "            'h', 'i', 'j', 'k', \n",
    "            'l', 'm', 'n', 'o', \n",
    "            'p', 'q', 'r', 's', \n",
    "            't', 'u', 'v', 'w', \n",
    "            'x', 'y', 'z', 'á', \n",
    "            'ã', 'ä', 'ç', 'è', \n",
    "            'é', 'ë', 'ï', 'ô', \n",
    "            'ö', 'ü', '-'\n",
    "        ])\n",
    "\n",
    "        self.personal_titles = set([\n",
    "            'dr', 'esq', 'hon', 'jr', \n",
    "            'mr', 'mrs', 'ms', 'messrs', \n",
    "            'mmes', 'msgr', 'prof', 'rev', \n",
    "            'rt', 'sr', 'st'\n",
    "        ])\n",
    "\n",
    "        # Map allowed chars to the index above\n",
    "        self.allowed_chars_in_filtered_names_to_index = {}\n",
    "        for i in range(len(self.allowed_chars_in_filtered_name)):\n",
    "            self.allowed_chars_in_filtered_names_to_index[self.allowed_chars_in_filtered_name[i]] = i\n",
    "\n",
    "        # We now want to map label to index, and index to label\n",
    "        self.label_to_index = {}\n",
    "        self.index_to_label = {}\n",
    "        \n",
    "        for i in range(len(possible_labels)):\n",
    "            label = possible_labels[i]\n",
    "            self.label_to_index[label] = i\n",
    "            self.index_to_label[i] = label\n",
    "\n",
    "        self.input_dimensions = len(self.allowed_chars_in_filtered_name)\n",
    "        self.target_dimensions = len(possible_labels)\n",
    "\n",
    "    '''\n",
    "        Puts the examples into an array of chars, with each char being a 28 bit array, \n",
    "        and labels into a bit array\n",
    "    '''\n",
    "    def serialize_examples_and_labels(self, examples, labels):\n",
    "        if len(examples) != len(labels):\n",
    "            raise Exception('Number of examples does not match number of labels!')\n",
    "\n",
    "        serialized_examples = []\n",
    "        serialized_labels = []\n",
    "\n",
    "        for i in range(len(examples)):\n",
    "            example = examples[i]\n",
    "            label = labels[i]\n",
    "            serialized_example = self.serialize_example(example)\n",
    "            serialized_label = self.serialize_label(label)\n",
    "\n",
    "            if serialized_example is not None and serialized_label is not None:\n",
    "                serialized_examples.append(serialized_example)\n",
    "                serialized_labels.append(serialized_label)\n",
    "\n",
    "        print('serialized', len(serialized_examples), 'examples')\n",
    "        print('serialized', len(serialized_labels), 'labels')\n",
    "\n",
    "        return np.array(serialized_examples), np.array(serialized_labels)\n",
    "                \n",
    "    '''\n",
    "        It converts a label into a binary form\n",
    "        For example, if we have self.label_to_index as:\n",
    "        {'US': 0, 'Canada': 1, 'Mexico': 2, 'Europe': 3}\n",
    "\n",
    "        and the label to be 'Mexico', it will return:\n",
    "        [0, 0, 1, 0].\n",
    "\n",
    "        Note that the length of the binary array will depend on the number of\n",
    "        keys in self.label_to_index\n",
    "    '''\n",
    "    def serialize_label(self, label):\n",
    "        if label in self.label_to_index:\n",
    "            index = self.label_to_index[label]\n",
    "            expected_val = np.zeros(self.target_dimensions)\n",
    "            expected_val[index] = 1\n",
    "            \n",
    "            return expected_val\n",
    "        else:\n",
    "            raise Exception('The label', label, 'does not exist in', self.label_to_index)\n",
    "\n",
    "    '''\n",
    "        Given an example with string 'abc', it will return:\n",
    "        [\n",
    "            [1, 0, 0, 0, ..., 0],\n",
    "            [0, 1, 0, 0, ..., 0],\n",
    "            [0, 0, 1, 0, ..., 0]\n",
    "        ]\n",
    "    '''\n",
    "    def serialize_example(self, example):\n",
    "        filtered_char = self._filter_chars_(example)\n",
    "        if filtered_char is None:\n",
    "            return None\n",
    "\n",
    "        name_array = []\n",
    "        for letter in filtered_char:\n",
    "            ascii_code = ord(letter)\n",
    "            letter_array = np.zeros(self.input_dimensions, )\n",
    "\n",
    "            if letter in self.allowed_chars_in_filtered_names_to_index:\n",
    "                letter_array[self.allowed_chars_in_filtered_names_to_index[letter]] = 1\n",
    "            else:\n",
    "                raise Exception(\"Illegal character in name:\", letter)\n",
    "\n",
    "            name_array.append(letter_array)\n",
    "\n",
    "        return np.array(name_array)\n",
    "\n",
    "    def _filter_chars_(self, example):\n",
    "        unfiltered_example = example\n",
    "\n",
    "        # Make letters all lowercase\n",
    "        # Ex: Mrs. John Smith -> mrs. john smith\n",
    "        example = example.lower()\n",
    "\n",
    "        # Remove non-space and non-letter characters\n",
    "        # Ex: mrs. john smith -> mrs john smith\n",
    "        filtered_example = ''\n",
    "        for c in example:\n",
    "            if c in self.allowed_chars_in_name:\n",
    "                filtered_example += c\n",
    "        example = filtered_example\n",
    "\n",
    "        # Remove duplicated spaces\n",
    "        # Ex: john  smith -> john smith\n",
    "        example = example.split()\n",
    "        new_example = ''\n",
    "        for c in example:\n",
    "            new_example += c + ' '\n",
    "        example = new_example[0:-1]\n",
    "\n",
    "        # Remove personal titles\n",
    "        # Ex: mr john smith -> john smith\n",
    "        example = example.split()\n",
    "        new_example = ''\n",
    "        for c in example:\n",
    "            if c not in self.personal_titles:\n",
    "                new_example += c + ' '\n",
    "        example = new_example[0:-1]\n",
    "\n",
    "        # Reject those with no characters\n",
    "        if len(example) == 0 or len(example.split()) == 0:\n",
    "            return None\n",
    "\n",
    "        # Reject those whose first or last name is only one letter\n",
    "        tokenized_example = example.split()\n",
    "        if len(tokenized_example) == 0 or len(tokenized_example[0]) <= 1 or len(tokenized_example[-1]) <= 1:\n",
    "            return None\n",
    "\n",
    "        # Remove names with single letters\n",
    "        # Ex: john n smith -> john smith\n",
    "        example = example.split()\n",
    "        new_example = ''\n",
    "        for c in example:\n",
    "            if len(c) > 1:\n",
    "                new_example += c + ' '\n",
    "        example = new_example[0:-1]\n",
    "\n",
    "        tokenized_example = example.split()\n",
    "\n",
    "        # Needs to contain only first and last name\n",
    "        # if len(tokenized_example) != 2:\n",
    "        #     return None\n",
    "\n",
    "\n",
    "        # Obtain the last name\n",
    "        # example = tokenized_example[-1]\n",
    "        # if len(tokenized_example) <= 1:\n",
    "        #     return None\n",
    "\n",
    "        # Needs to contain at least the first and last name\n",
    "        if len(tokenized_example) < 2:\n",
    "            return None\n",
    "\n",
    "        # final_example = tokenized_example[-1]\n",
    "        # return final_example\n",
    "\n",
    "        # print('OK')\n",
    "\n",
    "        # Add '$' in between text, and add '+' at the beginning and end of last name\n",
    "        # final_example = ''\n",
    "        # for i in range(len(tokenized_example) - 1):\n",
    "        #     final_example += '$' + tokenized_example[i] + '$ '\n",
    "        # final_example += '+' + tokenized_example[-1] + '+'\n",
    "\n",
    "        # Concat the final results\n",
    "        # final_example = ''\n",
    "        # for i in range(len(tokenized_example) - 1):\n",
    "        #     final_example += tokenized_example[i] + ' '\n",
    "        # final_example += tokenized_example[-1]\n",
    "\n",
    "        # Get only the first and last name\n",
    "        final_example = tokenized_example[0] + ' ' + tokenized_example[-1]\n",
    "\n",
    "        # print('Example:', unfiltered_example, '-> \"' + final_example + '\"')\n",
    "\n",
    "        return final_example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "approximate-mistress",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "'''\n",
    "    This contains useful activation functions\n",
    "'''\n",
    "class ActivationFunctions:\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid_derivative_given_sigmoid_val(sigmoid_value):\n",
    "\t    return sigmoid_value * (1 - sigmoid_value)\n",
    "\n",
    "    @staticmethod\n",
    "    def tanh(x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def tanh_derivative_given_tanh_val(tanh_value):\n",
    "        return 1.0 - (tanh_value ** 2)\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / np.sum(e_x, axis=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax_derivative(val):\n",
    "        softmax_val = ActivationFunctions.softmax(val)\n",
    "        reshaped_softmax_val = softmax_val.reshape(-1,1)\n",
    "        return np.diagflat(reshaped_softmax_val) - np.dot(reshaped_softmax_val, reshaped_softmax_val.T)\n",
    "\n",
    "'''\n",
    "    This contains useful loss functions\n",
    "'''\n",
    "class LossFunctions:\n",
    "    @staticmethod\n",
    "    def cross_entropy(hypothesis, expected_result, epsilon=1e-12):\n",
    "        return -np.sum(np.multiply(expected_result, np.log(hypothesis + epsilon)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "appreciated-scanning",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import random \n",
    "from sklearn.utils import shuffle\n",
    "from ml_utils import ActivationFunctions, LossFunctions\n",
    "import time\n",
    "from serializer import Serializer\n",
    "\n",
    "class NamesToNationalityClassifier:\n",
    "\n",
    "    def __init__(self, possible_labels, alpha=0.0001, hidden_dimensions=500, l2_lambda = 0.02, momentum=0.9, num_epoche=30):\n",
    "        self.serializer = Serializer(possible_labels)\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.input_dimensions = self.serializer.input_dimensions\n",
    "        self.hidden_dimensions = hidden_dimensions\n",
    "        self.output_dimensions = self.serializer.target_dimensions\n",
    "        self.training_to_validation_ratio = 0.7 # This means 70% of the dataset will be used for training, and 30% is for validation\n",
    "\n",
    "        # Weight Initialization\n",
    "        # We are using the Xavier initialization\n",
    "        # Reference: https://medium.com/usf-msds/deep-learning-best-practices-1-weight-initialization-14e5c0295b94\n",
    "        self.weight_init_type = 'X1'\n",
    "        self.W0 = np.random.randn(self.hidden_dimensions, self.hidden_dimensions) * np.sqrt(1 / self.hidden_dimensions)\n",
    "        self.W1 = np.random.randn(self.hidden_dimensions, self.input_dimensions + 1) * np.sqrt(1 / (self.input_dimensions + 1))\n",
    "        self.W2 = np.random.randn(self.output_dimensions, self.hidden_dimensions + 1) * np.sqrt(1 / (self.hidden_dimensions + 1))\n",
    "\n",
    "        # Momentum and regularization\n",
    "        self.l2_lambda = l2_lambda # The lambda for L2 regularization\n",
    "        self.momentum = momentum\n",
    "        self.W0_velocity = np.zeros((self.hidden_dimensions, self.hidden_dimensions))\n",
    "        self.W1_velocity = np.zeros((self.hidden_dimensions, self.input_dimensions + 1))\n",
    "        self.W2_velocity = np.zeros((self.output_dimensions, self.hidden_dimensions + 1))\n",
    "\n",
    "        # Bias values\n",
    "        self.layer_1_bias = 1\n",
    "        self.layer_2_bias = 1\n",
    "\n",
    "        # Num epoche\n",
    "        self.num_epoche = num_epoche\n",
    "\n",
    "        self.serialized_training_examples = []\n",
    "        self.serialized_training_labels = []\n",
    "        self.serialized_testing_examples = []\n",
    "        self.serialized_testing_labels = []\n",
    "\n",
    "    \n",
    "    def add_training_examples(self, examples, labels):\n",
    "        serialized_examples, serialized_labels = self.serializer.serialize_examples_and_labels(examples, labels) #self.__serialize_examples_and_labels__(examples, labels)\n",
    "        num_training_data = int(len(serialized_examples) * self.training_to_validation_ratio)\n",
    "\n",
    "        self.serialized_training_examples = serialized_examples[:num_training_data]\n",
    "        self.serialized_training_labels = serialized_labels[:num_training_data]\n",
    "        self.serialized_testing_examples = serialized_examples[num_training_data:]\n",
    "        self.serialized_testing_labels = serialized_labels[num_training_data:]\n",
    "\n",
    "    '''\n",
    "        Trains the model based on the training data provided.\n",
    "        It will output a dictionary with the following keys:\n",
    "        {\n",
    "            'epoche_to_train_avg_error': the train avg error per epoche,\n",
    "            'epoche_to_test_avg_error': the test avg error per epoche,\n",
    "            'epoche_to_train_accuracy': the train accuracy per epoche,\n",
    "            'epoche_to_test_accuracy': the test accuracy per epoche\n",
    "        }\n",
    "    '''\n",
    "    def train(self):\n",
    "        print(\"Training...\")\n",
    "        print(self)\n",
    "\n",
    "        epoche_to_train_avg_error = np.zeros((self.num_epoche, ))\n",
    "        epoche_to_test_avg_error = np.zeros((self.num_epoche, ))\n",
    "        epoche_to_train_accuracy = np.zeros((self.num_epoche, ))\n",
    "        epoche_to_test_accuracy = np.zeros((self.num_epoche, ))\n",
    "\n",
    "        for epoche in range(self.num_epoche):\n",
    "            train_avg_error = 0\n",
    "            train_accuracy = 0\n",
    "\n",
    "            # Reshuffle the data\n",
    "            self.serialized_training_examples, self.serialized_training_labels = shuffle(\n",
    "                self.serialized_training_examples, self.serialized_training_labels)\n",
    "\n",
    "            for i in range(len(self.serialized_training_examples)):\n",
    "\n",
    "                # It is a \"num_char\" x \"self.input_dimensions\" matrix\n",
    "                example = self.serialized_training_examples[i]\n",
    "\n",
    "                # It is a 1D array with \"self.output_dimensions\" elements\n",
    "                label = self.serialized_training_labels[i] \n",
    "\n",
    "                # Perform forward propagation\n",
    "                forward_propagation_results = self.__perform_forward_propagation__(example, label)\n",
    "                letter_pos_to_hypothesis = forward_propagation_results['letter_pos_to_hypothesis']\n",
    "                letter_pos_to_loss = forward_propagation_results['letter_pos_to_loss']\n",
    "\n",
    "                # Calculate the train avg error and the train accuracy\n",
    "                train_avg_error += np.sum(letter_pos_to_loss)\n",
    "                train_accuracy += 1 if self.__is_hypothesis_correct__(letter_pos_to_hypothesis[-1], label) else 0\n",
    "\n",
    "                # Perform back propagation\n",
    "                self.__perform_back_propagation__(example, label, forward_propagation_results)\n",
    "\n",
    "            epoche_to_train_avg_error[epoche] = train_avg_error / len(self.serialized_training_examples)\n",
    "            epoche_to_train_accuracy[epoche] = train_accuracy / len(self.serialized_training_examples)\n",
    "\n",
    "            test_avg_error, test_accuracy, test_runnable_ratio = self.__validate__()\n",
    "            epoche_to_test_accuracy[epoche] = test_accuracy\n",
    "            epoche_to_test_avg_error[epoche] = test_avg_error\n",
    "\n",
    "            print(epoche, epoche_to_train_avg_error[epoche], epoche_to_test_avg_error[epoche], epoche_to_train_accuracy[epoche], epoche_to_test_accuracy[epoche], test_runnable_ratio, time.time())\n",
    "\n",
    "        return {\n",
    "            'epoche_to_train_avg_error': epoche_to_train_avg_error,\n",
    "            'epoche_to_test_avg_error': epoche_to_test_avg_error,\n",
    "            'epoche_to_train_accuracy': epoche_to_train_accuracy,\n",
    "            'epoche_to_test_accuracy': epoche_to_test_accuracy\n",
    "        }\n",
    "\n",
    "    '''\n",
    "        Trains an example with a label.\n",
    "        The example is a name (like \"Bob Smith\") and its label is a country name (ex: \"Canada\")\n",
    "    '''\n",
    "    def train_example(self, example, label):\n",
    "        serialized_example = self.serializer.serialize_example(example)\n",
    "        serialized_label = self.serializer.serialize_label(label)\n",
    "\n",
    "        # Perform forward propagation\n",
    "        forward_propagation_results = self.__perform_forward_propagation__(serialized_example, serialized_label)\n",
    "\n",
    "        # Perform back propagation\n",
    "        self.__perform_back_propagation__(serialized_example, serialized_label, forward_propagation_results)\n",
    "\n",
    "    '''\n",
    "        It computes how well the model runs based on the validation data\n",
    "        It returns the avg. error and accuracy rate\n",
    "    '''\n",
    "    def __validate__(self):\n",
    "        total_cost = 0\n",
    "        num_correct = 0\n",
    "        num_examples_ran = 0\n",
    "\n",
    "        for i in range(len(self.serialized_testing_examples)):\n",
    "\n",
    "            # It is a num_char x 27 matrix\n",
    "            example = self.serialized_testing_examples[i]\n",
    "\n",
    "            # It is a 1D 124 element array\n",
    "            label = self.serialized_testing_labels[i] \n",
    "\n",
    "            forward_propagation_results = self.__perform_forward_propagation__(example, label)\n",
    "            letter_pos_to_loss = forward_propagation_results['letter_pos_to_loss']\n",
    "            letter_pos_to_hypothesis = forward_propagation_results['letter_pos_to_hypothesis']\n",
    "\n",
    "            if len(letter_pos_to_hypothesis) > 0:\n",
    "                final_hypothesis = letter_pos_to_hypothesis[-1]\n",
    "\n",
    "                # Seeing whether the hypothesis is correct\n",
    "                if self.__is_hypothesis_correct__(final_hypothesis, label):\n",
    "                    num_correct += 1\n",
    "\n",
    "                total_cost += np.sum(letter_pos_to_loss)\n",
    "\n",
    "                num_examples_ran += 1\n",
    "\n",
    "        avg_cost = total_cost / num_examples_ran\n",
    "        accuracy = num_correct / num_examples_ran\n",
    "        runnable_examples_ratio = num_examples_ran / len(self.serialized_testing_examples)\n",
    "\n",
    "        return avg_cost, accuracy, runnable_examples_ratio\n",
    "\n",
    "    def __is_hypothesis_correct__(self, hypothesis, label):\n",
    "        return np.argmax(hypothesis, axis=0) == np.argmax(label, axis=0)\n",
    "\n",
    "    '''\n",
    "        This function will perform a forward propagation with the serialized version of the example\n",
    "        and the serialized version of the label.\n",
    "\n",
    "        The serialized_example needs to be a 2D matrix with size num_char x self.input_dimensions.\n",
    "        The serialized_label needs to be a 1D array with size self.output_dimentions.\n",
    "\n",
    "        So this function will return:\n",
    "        - the loss at each timestep (called 'letter_pos_to_loss')\n",
    "        - the hidden states at each timestep (called 'letter_pos_to_hidden_state')\n",
    "        - the layer 2 values at each timestep (called 'letter_pos_to_layer_2_values')\n",
    "        - the hypothesis at each timestep (called 'letter_pos_to_hypothesis')\n",
    "        - \n",
    "    '''\n",
    "    def __perform_forward_propagation__(self, serialized_example, serialized_label):\n",
    "        num_chars = len(serialized_example)\n",
    "\n",
    "        # Stores the hidden state for each letter position.\n",
    "        letter_pos_to_h0 = np.zeros((num_chars + 1, self.hidden_dimensions))\n",
    "\n",
    "        # Stores the layer 2 values for each letter position\n",
    "        letter_pos_to_h1 = np.zeros((num_chars, self.hidden_dimensions))\n",
    "\n",
    "        # Stores the hypothesis for each letter position\n",
    "        letter_pos_to_h2 = np.zeros((num_chars, self.output_dimensions))\n",
    "\n",
    "        # The hidden state for the first letter position is all 0s.\n",
    "        letter_pos_to_h0[0] = np.zeros(self.hidden_dimensions)\n",
    "\n",
    "        # The loss for each letter position\n",
    "        letter_pos_to_loss = np.zeros((num_chars, ))\n",
    "\n",
    "        for j in range(num_chars):\n",
    "            # The inputs\n",
    "            X = serialized_example[j]\n",
    "            X_with_bias = np.r_[[self.layer_1_bias], X] # <- We add a bias to the input. It is now a 28 element array\n",
    "            h0 = letter_pos_to_h0[j]\n",
    "\n",
    "            y1 = np.dot(self.W1, X_with_bias) + np.dot(self.W0, h0)\n",
    "            h1 = ActivationFunctions.tanh(y1)\n",
    "\n",
    "            # Adding the bias\n",
    "            h1_with_bias = np.r_[[self.layer_2_bias], h1]\n",
    "\n",
    "            y2 = np.dot(self.W2, h1_with_bias)\n",
    "            h2 = ActivationFunctions.softmax(y2)\n",
    "\n",
    "            # Update the dictionaries\n",
    "            letter_pos_to_h1[j] = h1\n",
    "            letter_pos_to_h2[j] = h2\n",
    "            letter_pos_to_h0[j + 1] = h1\n",
    "\n",
    "            letter_pos_to_loss[j] = LossFunctions.cross_entropy(h2, serialized_label)\n",
    "        \n",
    "        return {\n",
    "            'letter_pos_to_loss': letter_pos_to_loss,\n",
    "            'letter_pos_to_hidden_state': letter_pos_to_h0,\n",
    "            'letter_pos_to_layer_2_values': letter_pos_to_h1,\n",
    "            'letter_pos_to_hypothesis': letter_pos_to_h2\n",
    "        }\n",
    "\n",
    "    '''\n",
    "        Performs back propagation.\n",
    "        Note that it requires the results from self.__perform_forward_propagation__() on the same example\n",
    "        Note that the example needs to be a serialized example, and the label needs to be a serialized label\n",
    "    '''\n",
    "    def __perform_back_propagation__(self, serialized_example, serialized_label, forward_propagation_results):\n",
    "        letter_pos_to_h0 = forward_propagation_results['letter_pos_to_hidden_state']\n",
    "        letter_pos_to_h1 = forward_propagation_results['letter_pos_to_layer_2_values']\n",
    "        letter_pos_to_h2 = forward_propagation_results['letter_pos_to_hypothesis']\n",
    "        letter_pos_to_loss = forward_propagation_results['letter_pos_to_loss']\n",
    "\n",
    "        # The loss gradients w.r.t W0, W1, W2\n",
    "        dL_dW0 = np.zeros((self.hidden_dimensions, self.hidden_dimensions))\n",
    "        dL_dW1 = np.zeros((self.hidden_dimensions, self.input_dimensions + 1))\n",
    "        dL_dW2 = np.zeros((self.output_dimensions, self.hidden_dimensions + 1))\n",
    "\n",
    "        num_chars = len(serialized_example)\n",
    "\n",
    "        for j in range(num_chars - 1, -1, -1):\n",
    "            X = serialized_example[j]\n",
    "            X_with_bias = np.r_[[self.layer_1_bias], X]\n",
    "            \n",
    "            # This is a 1D array with \"self.hidden_dimensions\" elements\n",
    "            h0 = letter_pos_to_h0[j]                    \n",
    "\n",
    "            # This is a 1D array with \"self.hidden_dimensions\" elements\n",
    "            h1 = letter_pos_to_h1[j]\n",
    "\n",
    "            # Adding the bias\n",
    "            # This is a 1D array with \"self.hidden_dimensions + 1\" elements\n",
    "            h1_with_bias = np.r_[[self.layer_2_bias], h1]\n",
    "\n",
    "            # This is a 1D array with \"self.output_dimensions\" elements                    \n",
    "            h2 = letter_pos_to_h2[j]\n",
    "\n",
    "            # This is a 1D array with \"self.output_dimentions\" elements\n",
    "            # This is the derivative of y with respect to the cross entropy score\n",
    "            dL_dY2 = h2 - serialized_label\n",
    "\n",
    "            # This is a 1D array with \"self.hidden_dimensions + 1\" elements\n",
    "            dL_dH1 = np.dot(dL_dY2.T, self.W2)\n",
    "            dL_dY1 = np.multiply(dL_dH1, ActivationFunctions.tanh_derivative_given_tanh_val(h1_with_bias))\n",
    "\n",
    "            # We are removing the bias value\n",
    "            # So now it is a \"self.hidden_dimensions\" elements\n",
    "            dL_dY1 = dL_dY1[1:]\n",
    "\n",
    "            # We are not updating the weights of the bias value, so we are setting the changes for the bias weights to 0\n",
    "            # We are going to update the weights of the bias value later\n",
    "            dL_dW0 += np.dot(np.array([dL_dY1]).T, np.array([h0]))\n",
    "            dL_dW1 += np.dot(np.array([dL_dY1]).T, np.array([X_with_bias]))\n",
    "            dL_dW2 += np.dot(np.array([dL_dY2]).T, np.array([h1_with_bias]))\n",
    "\n",
    "        # Add regularization\n",
    "        dL_dW0 += self.l2_lambda * self.W0\n",
    "        dL_dW1 += self.l2_lambda * self.W1\n",
    "        dL_dW2 += self.l2_lambda * self.W2\n",
    "\n",
    "        # Add the velocity\n",
    "        self.W0_velocity = (self.momentum * self.W0_velocity) + (self.alpha * dL_dW0)\n",
    "        self.W1_velocity = (self.momentum * self.W1_velocity) + (self.alpha * dL_dW1)\n",
    "        self.W2_velocity = (self.momentum * self.W2_velocity) + (self.alpha * dL_dW2)\n",
    "\n",
    "        # Update weights\n",
    "        self.W0 -= self.W0_velocity\n",
    "        self.W1 -= self.W1_velocity\n",
    "        self.W2 -= self.W2_velocity\n",
    "\n",
    "    def predict(self, name):\n",
    "        # Serialize the name to a num_char x 27 matrix\n",
    "        example = self.serializer.serialize_example(name)\n",
    "        # num_chars = len(example)\n",
    "        label = np.zeros((self.output_dimensions, ))\n",
    "\n",
    "        forward_propagation_results = self.__perform_forward_propagation__(example, label)\n",
    "        letter_pos_to_y2 = forward_propagation_results['letter_pos_to_hypothesis']\n",
    "\n",
    "        if len(letter_pos_to_y2) > 0:\n",
    "            hypothesis = ActivationFunctions.softmax(letter_pos_to_y2[-1])\n",
    "            formatted_hypothesis = []\n",
    "            for k in range(self.output_dimensions):\n",
    "                formatted_hypothesis.append((hypothesis[k], self.serializer.index_to_label[k]))\n",
    "\n",
    "            formatted_hypothesis.sort(reverse=True)\n",
    "\n",
    "            return formatted_hypothesis\n",
    "        else:\n",
    "            raise Exception('Hypothesis cannot be obtained')\n",
    "\n",
    "    def save_model(self, filename):\n",
    "        np.savez_compressed(filename, \n",
    "            layer_1_weights=self.W1, \n",
    "            layer_2_weights=self.W2, \n",
    "            hidden_state_weights=self.W0)\n",
    "\n",
    "    def load_model_from_file(self, filename):\n",
    "        data = np.load(filename)\n",
    "        self.W1 = data['layer_1_weights']\n",
    "        self.W2 = data['layer_2_weights']\n",
    "        self.W0 = data['hidden_state_weights']\n",
    "\n",
    "    def __str__(self):\n",
    "        description = \"RNN with learning rate: {}, momentum: {}, L2 reg. rate: {}, Weight Init. Type: {}, Num. Epoche: {}\" \n",
    "        return description.format(self.alpha, \n",
    "                                  self.momentum, \n",
    "                                  self.l2_lambda, \n",
    "                                  self.weight_init_type, \n",
    "                                  self.num_epoche)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encouraging-wisdom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Countries Filepath: data/countries-without-usa-or-canada.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from names_to_nationality_classifier import NamesToNationalityClassifier\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Make matplotlib not interactive\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "'''\n",
    "    Obtains a map from country ID to country name.\n",
    "    For example,\n",
    "    {\n",
    "        5998: (\"United Kingdom\", \"British\"),\n",
    "        5978: (\"China\", \"Chinese\"),\n",
    "        ...\n",
    "    }\n",
    "'''\n",
    "def get_countries(filepath='data/countries.csv'):\n",
    "    country_id_to_country_name = {}\n",
    "    print('Countries Filepath:', filepath)\n",
    "\n",
    "    with open(filepath) as countries_file_reader:\n",
    "\n",
    "        line = countries_file_reader.readline()\n",
    "        while line:\n",
    "            tokenized_line = line.split(',')\n",
    "            if len(tokenized_line) == 3:\n",
    "                country_id = int(tokenized_line[0])\n",
    "                country_name = tokenized_line[1]\n",
    "                nationality = tokenized_line[2]\n",
    "\n",
    "                country_id_to_country_name[country_id] = (country_name, nationality)\n",
    "\n",
    "            line = countries_file_reader.readline()\n",
    "\n",
    "    return country_id_to_country_name\n",
    "\n",
    "'''\n",
    "    Obtains the records from the CSV file into a list.\n",
    "    For example,\n",
    "    [\n",
    "        (\"Bob Smith\", 5998),\n",
    "        (\"Xi Jinping\", 5978),\n",
    "        ...\n",
    "    ]\n",
    "'''\n",
    "def get_records(max_records_per_country=float(\"inf\")):\n",
    "\n",
    "    # We first put all the records from the file\n",
    "    raw_records = []\n",
    "    with open('data/records.csv') as reader:\n",
    "\n",
    "        line = reader.readline()\n",
    "        while line:\n",
    "            tokenized_line = line.split(',')\n",
    "\n",
    "            if len(tokenized_line) == 3:\n",
    "                name = tokenized_line[1]\n",
    "                country_of_birth_id = int(tokenized_line[2])\n",
    "                raw_records.append((name, country_of_birth_id))\n",
    "\n",
    "            line = reader.readline()\n",
    "\n",
    "    # Shuffle the raw records to remove the potential ordering in the file\n",
    "    np.random.shuffle(raw_records)\n",
    "\n",
    "    # We then add the records to our dataset ensuring that it meets the count\n",
    "    records = []\n",
    "    country_id_to_num_records = {}\n",
    "    for record in raw_records:\n",
    "        country_of_birth_id = record[1]\n",
    "\n",
    "        if country_of_birth_id not in country_id_to_num_records:\n",
    "            records.append(record)\n",
    "            country_id_to_num_records[country_of_birth_id] = 1\n",
    "\n",
    "        elif country_id_to_num_records[country_of_birth_id] < max_records_per_country:\n",
    "            records.append(record)\n",
    "            country_id_to_num_records[country_of_birth_id] += 1\n",
    "\n",
    "    return records\n",
    "\n",
    "'''\n",
    "    It will return three values:\n",
    "    1.  A list of all possible labels\n",
    "        For example,\n",
    "        [\n",
    "            \"United Kingdom\", \n",
    "            \"China\", \n",
    "            ...\n",
    "        ]\n",
    "\n",
    "    2.  A list of examples\n",
    "        For example,\n",
    "        [\n",
    "            \"Bob Smith\",\n",
    "            \"Xi Jinping\",\n",
    "            ...\n",
    "        ]\n",
    "\n",
    "    3.  A list of labels where label[i] is the label for example[i]\n",
    "        For example,\n",
    "        [\n",
    "            \"United Kingdom\",\n",
    "            \"China\",\n",
    "            ...\n",
    "        ]\n",
    "\n",
    "    It returns in the order listed above\n",
    "\n",
    "    Note: For data/china-korea-japan-vietnam-countries.csv, use the following hyper-params:\n",
    "    - Momentum = 0.9\n",
    "    - L2 = 0.0001\n",
    "    - Learning Rate = 0.0001\n",
    "    - Hidden Dimensions: 200\n",
    "    - Epoche: 50\n",
    "\n",
    "    Note: For data/countries-without-usa-or-canada.csv, use the following hyper-params:\n",
    "    - Momentum = 0.9\n",
    "    - L2 = 0\n",
    "    - Learning Rate = 0.0001\n",
    "    - Hidden Dimensions: 500\n",
    "    - Epoche: 20\n",
    "'''\n",
    "def get_dataset():\n",
    "    country_id_to_country = get_countries(filepath='data/countries.csv')\n",
    "    #country_id_to_country = get_countries(filepath='data/countries-without-usa-or-canada.csv')\n",
    "    # country_id_to_country = get_countries(filepath='data/china-korea-japan-vietnam-countries.csv')\n",
    "    # country_id_to_country = get_countries(filepath='data/european-countries.csv')\n",
    "    countries = [ country_id_to_country[id][0] for id in country_id_to_country ]\n",
    "    countries.sort()\n",
    "\n",
    "    records = get_records(max_records_per_country=5000)\n",
    "    records = list(filter(lambda x: x[1] in country_id_to_country, records))\n",
    "    records = [( record[0], country_id_to_country[record[1]][0] ) for record in records]\n",
    "\n",
    "    # Shuffle the records\n",
    "    np.random.shuffle(records)\n",
    "        \n",
    "    # Splits the records into two lists\n",
    "    examples = [ record[0] for record in records ]\n",
    "    labels = [ record[1] for record in records ]\n",
    "\n",
    "    return countries, examples, labels\n",
    "\n",
    "'''\n",
    "    The main method\n",
    "'''\n",
    "def main():\n",
    "    countries, examples, labels = get_dataset()\n",
    "    plt.ioff()\n",
    "\n",
    "    classifier = NamesToNationalityClassifier(countries, \n",
    "                                            alpha=0.0001,\n",
    "                                            hidden_dimensions=500, \n",
    "                                            momentum=0.9,\n",
    "                                            num_epoche=20,\n",
    "                                            l2_lambda=0)\n",
    "\n",
    "    classifier.add_training_examples(examples, labels)\n",
    "    performance = classifier.train()\n",
    "\n",
    "    epoches = [i for i in range(classifier.num_epoche)]\n",
    "\n",
    "    # Plot the performance\n",
    "    fig, (errors_plt, accuracy_plt) = plt.subplots(2)\n",
    "    plt_title_format = \"Performance. for Learning Rate: {:.5f}, Hidden Dim: {:.5f}, \\nL2_lambda: {:.5f}, Momentum: {:.5f}, Num Epoche: {:.5f}\"\n",
    "    fig_title = plt_title_format.format(classifier.alpha, \n",
    "                                        classifier.hidden_dimensions, \n",
    "                                        classifier.l2_lambda, \n",
    "                                        classifier.momentum, \n",
    "                                        classifier.num_epoche)\n",
    "    fig.suptitle(fig_title, fontsize=10)\n",
    "\n",
    "    errors_plt.set_title('Errors vs Epoche', fontsize=10)\n",
    "    errors_plt.plot(epoches, performance['epoche_to_train_avg_error'], label='Train Avg. Error')\n",
    "    errors_plt.plot(epoches, performance['epoche_to_test_avg_error'], label='Test Avg. Error')\n",
    "    errors_plt.legend()\n",
    "    errors_plt.set_xlabel('Epoche')\n",
    "    errors_plt.set_ylabel('Error')\n",
    "\n",
    "    accuracy_plt.set_title('Accuracy vs Epoche', fontsize=10)\n",
    "    accuracy_plt.plot(epoches, performance['epoche_to_train_accuracy'], label='Train Accuracy')\n",
    "    accuracy_plt.plot(epoches, performance['epoche_to_test_accuracy'], label='Test Accuracy')\n",
    "    accuracy_plt.legend()\n",
    "    accuracy_plt.set_xlabel('Epoche')\n",
    "    accuracy_plt.set_ylabel('Accuracy')\n",
    "\n",
    "    plt.subplots_adjust(top=0.85)\n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "    # Save the plot\n",
    "    plt_file_name_format = 'L{}-H-{}-R-{}-M-{}-E-{}-plots.png'\n",
    "    plt_file_name = plt_file_name_format.format(classifier.weight_init_type,\n",
    "                                                str(classifier.hidden_dimensions).replace('.', '_'), \n",
    "                                                str(classifier.alpha).replace('.', '_'), \n",
    "                                                str(classifier.momentum).replace('.', '_'), \n",
    "                                                str(classifier.num_epoche).replace('.', '_'))\n",
    "    plt.savefig(plt_file_name)\n",
    "\n",
    "    # Save the data\n",
    "    data_file_name_format = 'L{}-H-{}-R-{}-M-{}-E-{}-data'\n",
    "    data_file_name = data_file_name_format.format(classifier.weight_init_type,\n",
    "                                                    str(classifier.hidden_dimensions).replace('.', '_'), \n",
    "                                                    str(classifier.alpha).replace('.', '_'), \n",
    "                                                    str(classifier.momentum).replace('.', '_'), \n",
    "                                                    str(classifier.num_epoche).replace('.', '_'))\n",
    "    print('Saved model to', data_file_name + '.npz')\n",
    "    classifier.save_model('data/' + data_file_name)\n",
    "\n",
    "    # # Train the model\n",
    "    # classifier = NamesToNationalityClassifier(countries)\n",
    "    # try:\n",
    "    #     print('Training data')\n",
    "    #     classifier.add_training_examples(examples, labels)\n",
    "    #     classifier.train()\n",
    "    # finally:\n",
    "    #     print('Saved model to data.npz')\n",
    "    #     classifier.save_model('data/data')\n",
    "\n",
    "    # Make predictions\n",
    "    # classifier.load_model_from_file('data/data.npz')\n",
    "    # print(classifier.predict('Emilio Kartono'))\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worth-romania",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
